{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66160f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sleep deprivation go brrrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea3e1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "295d0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5bf637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_blacklist=['_','/','#','.',',','?','[',']','-',':','@','\"',\"'\",'_','0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def pd_tokenize(pandas_series, blacklist=default_blacklist):\n",
    "    tokenized=[]\n",
    "    for _ in pandas_series:\n",
    "        for chara in blacklist:\n",
    "            _=_.replace(chara,'')\n",
    "        tokens=word_tokenize(_.lower())\n",
    "        tokenized.append(tokens)\n",
    "    return(pd.Series(tokenized))\n",
    "\n",
    "def pd_clean(pandas_series, blacklist=default_blacklist):\n",
    "    tokenized=[]\n",
    "    for _ in pandas_series:\n",
    "        for chara in blacklist:\n",
    "            _=_.replace(chara,'')\n",
    "        tokens=word_tokenize(_.lower())\n",
    "        tokenized.append(' '.join(tokens))\n",
    "    return(pd.Series(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "320f831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=pd_tokenize(train.text)\n",
    "cleaned=pd_clean(train.text)\n",
    "train['tokens']=tokens\n",
    "train['cleaned']=cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3e15a168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ariaahrary, thetawniest, the, out, of, contro...</td>\n",
       "      <td>ariaahrary thetawniest the out of control wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[m, utckm, s, of, volcano, hawaii, httptcozdto...</td>\n",
       "      <td>m utckm s of volcano hawaii httptcozdtoydebj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigating, after, an, ebike, coll...</td>\n",
       "      <td>police investigating after an ebike collided w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, latest, more, homes, razed, by, northern...</td>\n",
       "      <td>the latest more homes razed by northern califo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [our, deeds, are, the, reason, of, this, earth...   \n",
       "1         [forest, fire, near, la, ronge, sask, canada]   \n",
       "2     [all, residents, asked, to, shelter, in, place...   \n",
       "3     [people, receive, wildfires, evacuation, order...   \n",
       "4     [just, got, sent, this, photo, from, ruby, ala...   \n",
       "...                                                 ...   \n",
       "7608  [two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [ariaahrary, thetawniest, the, out, of, contro...   \n",
       "7610  [m, utckm, s, of, volcano, hawaii, httptcozdto...   \n",
       "7611  [police, investigating, after, an, ebike, coll...   \n",
       "7612  [the, latest, more, homes, razed, by, northern...   \n",
       "\n",
       "                                                cleaned  \n",
       "0     our deeds are the reason of this earthquake ma...  \n",
       "1                 forest fire near la ronge sask canada  \n",
       "2     all residents asked to shelter in place are be...  \n",
       "3     people receive wildfires evacuation orders in ...  \n",
       "4     just got sent this photo from ruby alaska as s...  \n",
       "...                                                 ...  \n",
       "7608  two giant cranes holding a bridge collapse int...  \n",
       "7609  ariaahrary thetawniest the out of control wild...  \n",
       "7610       m utckm s of volcano hawaii httptcozdtoydebj  \n",
       "7611  police investigating after an ebike collided w...  \n",
       "7612  the latest more homes razed by northern califo...  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "661510d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train0=train[train['target']==0].copy()\n",
    "train1=train[train['target']==1].copy()\n",
    "train0.reset_index(inplace=True)\n",
    "train1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde7be9",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c449adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv0=CountVectorizer(stop_words='english')\n",
    "cv1=CountVectorizer(stop_words='english')\n",
    "\n",
    "xcv0=cv0.fit_transform(train0.cleaned)\n",
    "xcv1=cv1.fit_transform(train1.cleaned)\n",
    "\n",
    "xcva0=xcv0.toarray()\n",
    "xcva1=xcv1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5bbbb3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_series_0=pd.Series(name=\"count\", data=0, index=cv0.get_feature_names())\n",
    "for tnum in range(0,len(xcva0)):\n",
    "    s=pd.Series(name=\"Count 0\", data=xcva0[tnum], index=cv0.get_feature_names())\n",
    "    s=s[s>0]\n",
    "    s=pd.DataFrame(s)\n",
    "    s.reset_index(inplace=True)\n",
    "    for il in range(len(s)):\n",
    "        tweet_series_0.at[s['index'].iloc[il]]+=1\n",
    "\n",
    "        \n",
    "tweet_series_1=pd.Series(name=\"count\", data=0, index=cv1.get_feature_names())\n",
    "for tnum in range(0,len(xcva1)):\n",
    "    s=pd.Series(name=\"Count 1\", data=xcva1[tnum], index=cv1.get_feature_names())\n",
    "    s=s[s>0]\n",
    "    s=pd.DataFrame(s)\n",
    "    s.reset_index(inplace=True)\n",
    "    for il in range(len(s)):\n",
    "        tweet_series_1.at[s['index'].iloc[il]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "257874ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_blacklist=['http','https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a08088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "83d4f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts0=tweet_series_0.copy()\n",
    "ts0.sort_values(ascending=False, inplace=True)\n",
    "ts0=pd.DataFrame(ts0)\n",
    "ts0.reset_index(inplace=True)\n",
    "\n",
    "ts1=tweet_series_1.copy()\n",
    "ts1.sort_values(ascending=False, inplace=True)\n",
    "ts1=pd.DataFrame(ts1)\n",
    "ts1.reset_index(inplace=True)\n",
    "\n",
    "f=open('disaster_1000_words.csv','w')\n",
    "f.write('word,count,percent_frequency')\n",
    "for _ in range(1000):\n",
    "    f.write(ts1.iloc[_]['index']+','+str(ts1.iloc[_]['count'])+','+str(ts1.iloc[_]['count']/len(ts1)*100)[0:6]+'%,\\n')\n",
    "f.close()\n",
    "\n",
    "f=open('safe_1000_words.csv','w')\n",
    "f.write('word,count,percent_frequency')\n",
    "for _ in range(1000):\n",
    "    f.write(ts0.iloc[_]['index']+','+str(ts0.iloc[_]['count'])+','+str(ts0.iloc[_]['count']/len(ts0)*100)[0:6]+'%,\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413a928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0b50e1b",
   "metadata": {},
   "source": [
    "# TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "866d7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf0=TfidfVectorizer(stop_words='english')\n",
    "tf1=TfidfVectorizer(stop_words='english')\n",
    "\n",
    "xtf0=tf0.fit_transform(train0.cleaned)\n",
    "xtf1=tf1.fit_transform(train1.cleaned)\n",
    "\n",
    "xtfa0=xtf0.toarray()\n",
    "xtfa1=xtf1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6e6afac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_series_0_tf=pd.Series(name=\"count\", data=0, index=tf0.get_feature_names())\n",
    "for tnum in range(0,len(xtfa0)):\n",
    "    s=pd.Series(name=\"Count 0\", data=xtfa0[tnum], index=tf0.get_feature_names())\n",
    "    s=s[s>0]\n",
    "    s=pd.DataFrame(s)\n",
    "    s.reset_index(inplace=True)\n",
    "    for il in range(len(s)):\n",
    "        tweet_series_0_tf.at[s['index'].iloc[il]]+=1\n",
    "\n",
    "        \n",
    "tweet_series_1_tf=pd.Series(name=\"count\", data=0, index=tf1.get_feature_names())\n",
    "for tnum in range(0,len(xtfa1)):\n",
    "    s=pd.Series(name=\"Count 1\", data=xtfa1[tnum], index=tf1.get_feature_names())\n",
    "    s=s[s>0]\n",
    "    s=pd.DataFrame(s)\n",
    "    s.reset_index(inplace=True)\n",
    "    for il in range(len(s)):\n",
    "        tweet_series_1_tf.at[s['index'].iloc[il]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3a62dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts0=tweet_series_0_tf.copy()\n",
    "ts0.sort_values(ascending=False, inplace=True)\n",
    "ts0=pd.DataFrame(ts0)\n",
    "ts0.reset_index(inplace=True)\n",
    "\n",
    "ts1=tweet_series_1_tf.copy()\n",
    "ts1.sort_values(ascending=False, inplace=True)\n",
    "ts1=pd.DataFrame(ts1)\n",
    "ts1.reset_index(inplace=True)\n",
    "\n",
    "f=open('disaster_1000_words_tf.csv','w')\n",
    "f.write('word,total_importance,importance_proportion')\n",
    "for _ in range(1000):\n",
    "    f.write(ts1.iloc[_]['index']+','+str(ts1.iloc[_]['count'])+','+str(ts1.iloc[_]['count']/len(ts1))[0:6]+',\\n')\n",
    "f.close()\n",
    "\n",
    "f.write('word,total_importance,importance_proportion')\n",
    "for _ in range(1000):\n",
    "    f.write(ts0.iloc[_]['index']+','+str(ts0.iloc[_]['count'])+','+str(ts0.iloc[_]['count']/len(ts0))[0:6]+',\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed1832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d262fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "\n",
    "term_doc = mmread('../bbc/bbc.mtx')  # term-document matrix\n",
    "doc_term = term_doc.T  # document-term matrix\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf2 = NMF(n_components=2)\n",
    "doc_topic = nmf2.fit_transform(train.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
