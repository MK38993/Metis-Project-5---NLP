{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3e1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "\n",
    "import time\n",
    "from time import sleep, time\n",
    "from timeit import timeit\n",
    "\n",
    "import re\n",
    "from re import findall\n",
    "import string\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn.datasets as dt\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('u')\n",
    "stop_words.append('nt')\n",
    "\n",
    "\n",
    "# Edited from from https://www.digitalocean.com/community/tutorials/\n",
    "# how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "cases_dict={}\n",
    "#cases_dict['u']='you'\n",
    "cases_dict['ca']='california'\n",
    "cases_dict['bloody']='blood'\n",
    "cases_dict['burning']='burn'\n",
    "cases_dict['derailment']='derailment'\n",
    "cases_dict['explosion']='explode'\n",
    "cases_dict['evacuation']='evacuate'\n",
    "cases_dict['die']='death'\n",
    "cases_dict['dead']='death'\n",
    "cases_dict['collision']='collide'\n",
    "cases_dict['bomber']='bomb'\n",
    "cases_dict['destruction']='destroy'\n",
    "cases_dict['fatality']='fatal'\n",
    "cases_dict['gon']='gone'\n",
    "cases_dict['terrorist']='terrorism'\n",
    "cases_dict['thunder']='thunderstorm'\n",
    "cases_dict['wreck']='wreckage'\n",
    "cases_dict['wildfire']='fire'\n",
    "cases_dict['cop']='police'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def special_cases(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in cases_dict:\n",
    "            tokens[i]=cases_dict[tokens[i]]\n",
    "#     if tokens in cases_dict:\n",
    "#         tokens=cases_dict[tokens]\n",
    "    return(tokens)\n",
    "\n",
    "def unicodify(to_uni):\n",
    "    return(''.join(r'\\u{:04X}'.format(ord(chr)) for chr in to_uni))\n",
    "\n",
    "\n",
    "\n",
    "def listify(to_listify, uni=False):\n",
    "    if uni:\n",
    "        listed=findall(\"'(.+?)'\",to_listify)\n",
    "        for i in range(len(listed)):\n",
    "            listed[i]=unicodify(listed[i])\n",
    "        return(listed)\n",
    "    else:\n",
    "        return(findall(\"'(.+?)'\",to_listify))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_sentence(tokens): \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = stop_words):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|''(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token=re.sub('t.co[/a-z]+','',token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        token = re.sub(\"[',.0-9]\",\"\", token)\n",
    "        token=special_cases(token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        elif tag.startswith('JJ') or tag.startswith('NNP'):\n",
    "            pos = 'del'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if pos!='del':\n",
    "            token = lemmatizer.lemmatize(token, pos)\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words and pos!='adj':\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d33d5ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RECREATE_DATASET=False\n",
    "\n",
    "if RECREATE_DATASET:\n",
    "    train=pd.read_csv('train.csv')\n",
    "\n",
    "    #Clean/tokenize text\n",
    "    hyperlink_blocker=['http','//','amp','yr','gt','mh','na','th','nt','\\x89ûª','\\x89ûªs','\\x89ûò','\\x89û','fuck','shit']\n",
    "\n",
    "    f=open('blacklist.txt')\n",
    "    for item in f:\n",
    "        hyperlink_blocker.append(item.replace('\\n',''))\n",
    "    f.close()\n",
    "\n",
    "    tweet_list=train.text.tolist()\n",
    "    tweet_bigrams=[]\n",
    "    tweet_trigrams=[]\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        t=word_tokenize(tweet_list[i])\n",
    "        t=lemmatize_sentence(t)\n",
    "        t=remove_noise(t,stop_words)\n",
    "        t=special_cases(t)\n",
    "\n",
    "        block_count=0\n",
    "        for j in range(len(t)):\n",
    "            for item in hyperlink_blocker:\n",
    "                if item in t[j] or len(t[j])<3:\n",
    "                    block_count+=1\n",
    "                    t[j]='[[Hyperlink Blocked]]'\n",
    "        for j in range(block_count):\n",
    "            t.remove('[[Hyperlink Blocked]]')\n",
    "\n",
    "        tweet_list[i]=t\n",
    "        # Add bi/tri-grams in their separate lists\n",
    "        bi, tri=[],[]\n",
    "        for j in range(len(t)-1):\n",
    "            bi.append(str(t[j]+'_'+t[j+1]))\n",
    "            #if j<len(t)-2:\n",
    "                #tri.append(str(t[j]+'_'+t[j+1]+'_'+t[j+2]))\n",
    "\n",
    "        tweet_bigrams.append(bi)\n",
    "    train['tokens']=tweet_list\n",
    "    train['tokens2']=tweet_bigrams\n",
    "\n",
    "    #train['tokens3']=tweet_trigrams\n",
    "\n",
    "    ftl=[]\n",
    "    for i in range(len(tweet_list)):\n",
    "        ftl.append(\" \".join(tweet_list[i]))\n",
    "\n",
    "    train['cleaned']=ftl\n",
    "\n",
    "    train.fillna('N/A', inplace=True)\n",
    "\n",
    "    #Keep only tweets with more than 4 token words\n",
    "    deleter=[]\n",
    "    for i in range(len(train['tokens'])):\n",
    "        if len(train['tokens'].iloc[i])<=4:\n",
    "            deleter.append(True)\n",
    "        else:\n",
    "            deleter.append(False)\n",
    "    train['delet']=deleter\n",
    "\n",
    "    train=train[train['delet']!=True].reset_index().drop(['index'],axis=1)\n",
    "\n",
    "    pol=[]\n",
    "    sub=[]\n",
    "    for text in train.text:\n",
    "        pol.append(TextBlob(text).sentiment[0])\n",
    "        sub.append(TextBlob(text).sentiment[1])\n",
    "    train['pol']=pol\n",
    "    train['sub']=sub\n",
    "\n",
    "    allgrams=[]\n",
    "    for i in range(len(train['text'])):\n",
    "        allgrams.append(train['tokens'].iloc[i]+train['tokens2'].iloc[i])\n",
    "    train['tokens_all']=allgrams\n",
    "\n",
    "    train.to_csv('flattened_train.csv')\n",
    "    train=pd.read_csv('flattened_train.csv').drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "    train.drop_duplicates(subset='tokens',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb1c02",
   "metadata": {},
   "source": [
    "# Checkpoint after cleaning/tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train=False\n",
    "if save_train:\n",
    "    train.to_csv('train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2891cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_clean.csv').drop(['Unnamed: 0'],axis=1)\n",
    "train.fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbe0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target']==1].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0fa7b",
   "metadata": {},
   "source": [
    "# Run K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68278144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de335605",
   "metadata": {},
   "outputs": [],
   "source": [
    "RERUN_MODEL=False\n",
    "\n",
    "if RERUN_MODEL:\n",
    "\n",
    "    # X Clusters\n",
    "    KM=KMeans()\n",
    "    KM.fit(xtfa1)\n",
    "\n",
    "    cluster_nums=KM.predict(xtfa1)\n",
    "\n",
    "    plt.figure(figsize=[16,16])\n",
    "    plt.scatter(xtfa1_md[:,0],xtfa1_md[:,1],alpha=cluster_nums==0,c=cluster_nums)\n",
    "\n",
    "    train1['cluster']=cluster_nums\n",
    "    train1[train1['cluster']==2]\n",
    "\n",
    "    train1.drop(['cluster_20'],axis=1,inplace=True)\n",
    "\n",
    "    train=pd.read_csv('train_clean.csv').drop(['Unnamed: 0'],axis=1)\n",
    "    train.fillna('N/A', inplace=True)\n",
    "\n",
    "    xtfa[i].any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426262c",
   "metadata": {},
   "source": [
    "### I compared K-Means to LDA and LDA seems to be the better option here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b8e16",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09e82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RERUN_MODEL=True\n",
    "\n",
    "if RERUN_MODEL:\n",
    "    #train=pd.read_csv('train_clean.csv').drop(['Unnamed: 0'],axis=1)\n",
    "    #train.fillna('N/A', inplace=True)\n",
    "\n",
    "    #MAXDF=0.1\n",
    "    #MINDF=0.005\n",
    "\n",
    "    MAXDF=0.075\n",
    "    MINDF=0.005\n",
    "\n",
    "    last=-1\n",
    "    deleter=[]\n",
    "    while len(deleter)!=last:\n",
    "\n",
    "        tokens_all=train.tokens+train.tokens2\n",
    "\n",
    "        tf=TfidfVectorizer(stop_words=stop_words, max_df=MAXDF, min_df=MINDF,use_idf=False)\n",
    "        xtf=tf.fit_transform(tokens_all)\n",
    "        xtfa=xtf.toarray()\n",
    "        #print('!')\n",
    "        #throw()\n",
    "        deleter=[]\n",
    "        for i in range(len(xtfa)):\n",
    "            if not xtfa[i].any():\n",
    "                deleter.append(True)\n",
    "            else:\n",
    "                deleter.append(False)\n",
    "\n",
    "        train['delet']=deleter\n",
    "        train=train[train['delet']==False].reset_index().drop(['index'],axis=1)\n",
    "        last=len(deleter)\n",
    "        #print(last)\n",
    "\n",
    "    train1=train[train['target']==1].copy()\n",
    "    train1.reset_index(inplace=True)\n",
    "\n",
    "    tokens_1=train1.tokens+train1.tokens2\n",
    "\n",
    "\n",
    "    tf1=TfidfVectorizer(stop_words=stop_words, max_df=MAXDF, min_df=MINDF,use_idf=False)\n",
    "    xtf1=tf1.fit_transform(tokens_1)\n",
    "\n",
    "    xtfa1=xtf1.toarray()\n",
    "    xtffn1=tf1.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644ef5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtffn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d231e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if RERUN_MODEL:\n",
    "    cluster_topics=16\n",
    "\n",
    "    text_list=train1['tokens'].tolist()\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i]=listify(text_list[i])\n",
    "\n",
    "    dictionary=corpora.Dictionary(text_list)\n",
    "    doc_term_matrix = []\n",
    "\n",
    "    for doc in train1['tokens']:\n",
    "        doc_term_matrix.append(dictionary.doc2bow(listify(doc,True)))\n",
    "\n",
    "    doc_word = tf1.transform(tokens_all).transpose()\n",
    "    pd.DataFrame(doc_word.toarray(), tf1.get_feature_names()).head()\n",
    "\n",
    "    corpus = matutils.Sparse2Corpus(doc_word)\n",
    "    dictionary = corpora.Dictionary()\n",
    "    id2word = dict((v, k) for k, v in tf1.vocabulary_.items())\n",
    "\n",
    "    lda1 = models.LdaModel(corpus=corpus, num_topics=cluster_topics, id2word=id2word, passes=8)\n",
    "\n",
    "    topic_matrix1=lda1.get_topics()\n",
    "\n",
    "    cluster_scores=[]\n",
    "\n",
    "    for r in range(len(train1['tokens_all'])):\n",
    "        cluster_score=defaultdict(lambda:0)\n",
    "        for item in listify(train1['tokens_all'].iloc[r]):\n",
    "            if item in xtffn1:\n",
    "                tok_num=xtffn1.index(item)\n",
    "                for topic in range(len(topic_matrix1)):\n",
    "                    cluster_score[topic]+=topic_matrix1[topic][tok_num]\n",
    "        cluster_scores.append(cluster_score)\n",
    "\n",
    "    hard_cluster=[]\n",
    "    for defdict in cluster_scores:\n",
    "        try:\n",
    "            kmax = max(zip(defdict.values(), defdict.keys()))[1]\n",
    "            hard_cluster.append(kmax)\n",
    "        except:\n",
    "            hard_cluster.append(0)\n",
    "\n",
    "    train1=train1.reset_index().drop(['level_0'],axis=1)\n",
    "\n",
    "    train1['lda_cnum']=hard_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda1.show_topics(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190ba52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train1[train1['lda_cnum']==15].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab151f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553bde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics:\n",
    "\n",
    "# Fire\n",
    "# Storms/Flooding\n",
    "# Injury\n",
    "# Police\n",
    "# Accidents\n",
    "# Terrorism\n",
    "# War\n",
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train1.to_csv('train1.csv')\n",
    "\n",
    "#f = open('topic_matrix1', 'wb')\n",
    "#pickle.dump(topic_matrix1, f)\n",
    "#f.close()\n",
    "\n",
    "# f = open('xtffn1', 'wb')\n",
    "# pickle.dump(xtffn1, f)\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=pd.read_csv('train1.csv')\n",
    "\n",
    "f = open('topic_matrix1', 'rb') \n",
    "topic_matrix1=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('xtffn1', 'rb') \n",
    "xtffn1=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d335398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_tweet(tweet):\n",
    "    \n",
    "    cluster_names={0:'Misc',\n",
    "                1:'Fire',\n",
    "                2:'Injury',\n",
    "                3:'Police',\n",
    "                4:'Fire',\n",
    "                5:'Accidents',\n",
    "                6:'Misc',\n",
    "                7:'Injury',\n",
    "                8:'Terrorism',\n",
    "                9:'Storms/Flooding',\n",
    "                10:'Police',\n",
    "                11:'Misc',\n",
    "                12:'Terrorism',\n",
    "                13:'Storms/Flooding',\n",
    "                14:'Misc',\n",
    "                15:'War'}\n",
    "    print(f'Original tweet:{tweet}\\n')\n",
    "    t=word_tokenize(tweet)\n",
    "    t=lemmatize_sentence(t)         \n",
    "    t=special_cases(t)\n",
    "    print(f'Tokenized/Lemmatized sentence:{t}\\n')\n",
    "    \n",
    "    no_match=True\n",
    "    \n",
    "    cluster_score=defaultdict(lambda:0)\n",
    "    for item in t:\n",
    "        if item in xtffn1:\n",
    "            tok_num=xtffn1.index(item)\n",
    "            for topic in range(len(topic_matrix1)):\n",
    "                cluster_score[topic]+=topic_matrix1[topic][tok_num]\n",
    "                no_match=False\n",
    "    if no_match:\n",
    "        print('Error: Unable to sort into clusters. Try a different tweet.')\n",
    "        return('Error: Unable to sort into clusters.')\n",
    "    \n",
    "    list_form=[]\n",
    "    for item in cluster_score:\n",
    "        list_form.append(cluster_score[item])\n",
    "        \n",
    "    scale_factor=1/sum(list_form)\n",
    "    \n",
    "    out_dict=defaultdict(lambda:0)\n",
    "    \n",
    "    for idx in range(len(list_form)):\n",
    "        list_form[idx]*=scale_factor*100\n",
    "        out_dict[cluster_names[idx]]+=list_form[idx]\n",
    "\n",
    "    return(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_tweet():\n",
    "    return(train1.iloc[random.randint(0,len(train1.text))].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_index(idx):\n",
    "    return(train1.iloc[idx].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(0,\n",
    "#   '0.111*\"miss\" + 0.107*\"long\" + 0.101*\"survive\" + 0.071*\"shoot\" + 0.070*\"deluge\" + 0.064*\"follow\" + 0.063*\"calgary\" + 0.062*\"major\" + 0.059*\"food\" + 0.043*\"flame\"'),\n",
    "#  (1,\n",
    "#   '0.134*\"burn\" + 0.090*\"mass\" + 0.089*\"high\" + 0.087*\"school\" + 0.084*\"whole\" + 0.076*\"build\" + 0.070*\"move\" + 0.055*\"refugee\" + 0.044*\"affect\" + 0.043*\"abc\"'),\n",
    "#  (2,\n",
    "#   '0.181*\"wound\" + 0.092*\"movie\" + 0.077*\"fatal\" + 0.077*\"leave\" + 0.069*\"woman\" + 0.061*\"nuclear\" + 0.058*\"old\" + 0.049*\"well\" + 0.046*\"release\" + 0.039*\"end\"'),\n",
    "#  (3,\n",
    "#   '0.135*\"officer\" + 0.130*\"collapse\" + 0.087*\"late\" + 0.085*\"aug\" + 0.071*\"hour\" + 0.066*\"line\" + 0.062*\"casualty\" + 0.044*\"murderer\" + 0.040*\"crime\" + 0.039*\"bridge\"'),\n",
    "#  (4,\n",
    "#   '0.152*\"see\" + 0.126*\"wild\" + 0.093*\"wild_fire\" + 0.076*\"car\" + 0.076*\"let\" + 0.072*\"never\" + 0.064*\"call\" + 0.047*\"sign\" + 0.036*\"send\" + 0.036*\"catch\"'),\n",
    "#  (5,\n",
    "#   '0.133*\"help\" + 0.102*\"top\" + 0.094*\"near\" + 0.083*\"keep\" + 0.070*\"collide\" + 0.054*\"person\" + 0.050*\"early\" + 0.043*\"rise\" + 0.039*\"oil\" + 0.035*\"deal\"'),\n",
    "#  (6,\n",
    "#   '0.207*\"trauma\" + 0.093*\"much\" + 0.083*\"give\" + 0.081*\"big\" + 0.067*\"scream\" + 0.065*\"terrorism\" + 0.061*\"week\" + 0.059*\"riot\" + 0.058*\"hope\" + 0.053*\"happen\"'),\n",
    "#  (7,\n",
    "#   '0.099*\"blood\" + 0.086*\"really\" + 0.084*\"building\" + 0.069*\"crash\" + 0.069*\"photo\" + 0.067*\"save\" + 0.060*\"half\" + 0.049*\"lose\" + 0.041*\"drive\" + 0.041*\"fight\"'),\n",
    "#  (8,\n",
    "#   '0.112*\"man\" + 0.097*\"bomb\" + 0.093*\"tragedy\" + 0.069*\"city\" + 0.053*\"set\" + 0.043*\"suicide\" + 0.039*\"island\" + 0.039*\"plane\" + 0.033*\"suicide_bomb\" + 0.033*\"malaysia\"'),\n",
    "#  (9,\n",
    "#   '0.247*\"wreckage\" + 0.178*\"weapon\" + 0.054*\"typhoon\" + 0.047*\"wind\" + 0.045*\"hiroshima\" + 0.040*\"nuclear\" + 0.038*\"soudelor\" + 0.035*\"typhoon_soudelor\" + 0.035*\"breaking\" + 0.033*\"hear\"'),\n",
    "#  (10,\n",
    "#   '0.177*\"police\" + 0.111*\"home\" + 0.071*\"ever\" + 0.068*\"suspect\" + 0.049*\"area\" + 0.049*\"police_officer\" + 0.047*\"inside\" + 0.041*\"summer\" + 0.040*\"murder\" + 0.038*\"battle\"'),\n",
    "#  (11,\n",
    "#   '0.145*\"need\" + 0.116*\"body\" + 0.086*\"many\" + 0.081*\"evacuate\" + 0.057*\"family\" + 0.052*\"group\" + 0.052*\"tonight\" + 0.051*\"blow\" + 0.048*\"force\" + 0.047*\"land\"'),\n",
    "#  (12,\n",
    "#   '0.118*\"attack\" + 0.102*\"head\" + 0.087*\"fall\" + 0.083*\"train\" + 0.074*\"flood\" + 0.062*\"change\" + 0.058*\"road\" + 0.052*\"rain\" + 0.039*\"kill\" + 0.034*\"india\"'),\n",
    "#  (13,\n",
    "#   '0.110*\"even\" + 0.076*\"injury\" + 0.066*\"red\" + 0.058*\"rescue\" + 0.057*\"military\" + 0.054*\"damage\" + 0.050*\"fear\" + 0.047*\"victim\" + 0.044*\"plan\" + 0.043*\"severe\"'),\n",
    "#  (14,\n",
    "#   '0.115*\"great\" + 0.095*\"life\" + 0.088*\"explode\" + 0.080*\"live\" + 0.070*\"night\" + 0.070*\"hit\" + 0.051*\"heat\" + 0.045*\"water\" + 0.042*\"place\" + 0.042*\"wave\"'),\n",
    "#  (15,\n",
    "#   '0.179*\"war\" + 0.116*\"emergency\" + 0.083*\"kill\" + 0.055*\"talk\" + 0.055*\"loud\" + 0.050*\"service\" + 0.044*\"large\" + 0.035*\"across\" + 0.034*\"morning\" + 0.032*\"survivor\"')]\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "#   cluster_names={0:'Misc',\n",
    "#                 1:'Fire',\n",
    "#                 2:'Injury',\n",
    "#                 3:'Police',\n",
    "#                 4:'Fire',\n",
    "#                 5:'Accidents',\n",
    "#                 6:'Misc',\n",
    "#                 7:'Injury',\n",
    "#                 8:'Terrorism',\n",
    "#                 9:'Storms/Flooding',\n",
    "#                 10:'Police',\n",
    "#                 11:'Misc',\n",
    "#                 12:'Terrorism',\n",
    "#                 13:'Storms/Flooding',\n",
    "#                 14:'Misc',\n",
    "#                 15:'War'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e823c15",
   "metadata": {},
   "source": [
    "# Display subjectivity/polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa0439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue='#1111CC'\n",
    "# orange='#ED8811'\n",
    "\n",
    "# plt.figure(figsize=[16,10])\n",
    "# plt.title('Sentiment of Tweets')\n",
    "# plt.xlabel('Polarity')\n",
    "# plt.ylabel('Subjectivity')\n",
    "# plt.scatter(train0['pol'],train0['sub'],alpha=0.25,color=blue)\n",
    "# plt.scatter(train1['pol'],train1['sub'],alpha=0.25,color=orange)\n",
    "# plt.legend(['\"Safe\"','\"Disaster\"'])\n",
    "\n",
    "# pol0=train0['pol'].mean()\n",
    "# pol1=train1['pol'].mean()\n",
    "# sub0=train0['sub'].mean()\n",
    "# sub1=train1['sub'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1975a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(train0['pol'])\n",
    "#plt.hist(train1['pol'],color='orange')\n",
    "\n",
    "#plt.hist(train0['sub'])\n",
    "#plt.hist(train1['sub'],color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar('Polarity of \"Safe\" Tweets',pol0,color=blue)\n",
    "# plt.bar('Polarity of \"Disaster\" Tweets',pol1,color=orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b95377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar('Subjectivity of \"Safe\" Tweets',sub0,color=blue)\n",
    "# plt.bar('Subjectivity of \"Disaster\" Tweets',sub1,color=orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ce154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# from re import findall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be205c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# toks=[]\n",
    "# for item in range(len(train1.tokens)):\n",
    "#     s=train1.tokens.iloc[item]+train1.tokens2.iloc[item]\n",
    "#     for tok in findall(\"'(.+?)'\",train1.tokens.iloc[item]):\n",
    "#         toks.append(tok)\n",
    "\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=99999, contour_width=3, contour_color='steelblue')\n",
    "# wordcloud.generate(','.join(toks))\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f025bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import mmread\n",
    "\n",
    "# term_doc = mmread('../bbc/bbc.mtx')  # term-document matrix\n",
    "# doc_term = term_doc.T  # document-term matrix\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "# nmf2 = NMF(n_components=2)\n",
    "# doc_topic = nmf2.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0073a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71bf3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d057f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da66f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192870a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9dec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5772e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec504c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007601ac",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05606708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old tokenization functions\n",
    "# def pd_tokenize(pandas_series, blacklist=default_blacklist):\n",
    "#     tokenized=[]\n",
    "#     for _ in pandas_series:\n",
    "#         for chara in blacklist:\n",
    "#             _=_.replace(chara,' ')\n",
    "#         tokens=word_tokenize(_.lower())\n",
    "#         hyperlink_blocked=[]\n",
    "#         for i in range(len(tokens)):\n",
    "#             if len(re.findall('http',tokens[i]))>0:\n",
    "#                 hyperlink_blocked.append(i)\n",
    "#         hyperlink_blocked.reverse()\n",
    "#         for i in hyperlink_blocked:\n",
    "#             tokens.pop(i)\n",
    "#         tokenized.append([lmt.lemmatize(word) for word in tokens])\n",
    "#     return(pd.Series(tokenized))\n",
    "\n",
    "# def pd_clean(pandas_series, blacklist=default_blacklist):\n",
    "#     tokenized=[]\n",
    "#     for _ in pandas_series:\n",
    "#         for chara in blacklist:\n",
    "#             _=_.replace(chara,' ')\n",
    "#         tokens=word_tokenize(_.lower())\n",
    "#         hyperlink_blocked=[]\n",
    "#         for i in range(len(tokens)):\n",
    "#             if len(re.findall('http',tokens[i]))>0:\n",
    "#                 hyperlink_blocked.append(i)\n",
    "#         hyperlink_blocked.reverse()\n",
    "#         for i in hyperlink_blocked:\n",
    "#             tokens.pop(i)\n",
    "#         tokenized.append(\" \".join([lmt.lemmatize(word) for word in tokens]))\n",
    "#     return(pd.Series(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to count total importance of words\n",
    "# count_importance=False\n",
    "# if count_importance:\n",
    "#     tweet_series_0_tf=pd.Series(name=\"count\", data=0.0, index=tf0.get_feature_names())\n",
    "#     for tnum in range(0,len(xtfa0)):\n",
    "#         s=pd.Series(name=\"count\", data=xtfa0[tnum], index=tf0.get_feature_names())\n",
    "#         s=s[s>0]\n",
    "#         s=pd.DataFrame(s)\n",
    "#         s.reset_index(inplace=True)\n",
    "#         for il in range(len(s)):\n",
    "#             tweet_series_0_tf.at[s['index'].iloc[il]]+=s.iloc[il]['count']\n",
    "\n",
    "\n",
    "#     tweet_series_1_tf=pd.Series(name=\"count\", data=0.0, index=tf1.get_feature_names())\n",
    "#     for tnum in range(0,len(xtfa1)):\n",
    "#         s=pd.Series(name=\"count\", data=xtfa1[tnum], index=tf1.get_feature_names())\n",
    "#         s=s[s>0]\n",
    "#         s=pd.DataFrame(s)\n",
    "#         s.reset_index(inplace=True)\n",
    "#         for il in range(len(s)):\n",
    "#             tweet_series_1_tf.at[s['index'].iloc[il]]+=s.iloc[il]['count']\n",
    "\n",
    "\n",
    "#     ts0=tweet_series_0_tf.copy()\n",
    "#     ts0.sort_values(ascending=False, inplace=True)\n",
    "#     ts0=pd.DataFrame(ts0)\n",
    "#     ts0.reset_index(inplace=True)\n",
    "\n",
    "#     ts1=tweet_series_1_tf.copy()\n",
    "#     ts1.sort_values(ascending=False, inplace=True)\n",
    "#     ts1=pd.DataFrame(ts1)\n",
    "#     ts1.reset_index(inplace=True)\n",
    "\n",
    "#     f=open('disaster_1000_words_tf.csv','w')\n",
    "#     f.write('word,total_importance,importance_proportion,\\n')\n",
    "#     for _ in range(1000):\n",
    "#         f.write(ts1.iloc[_]['index']+','+str(ts1.iloc[_]['count'])+','+str(ts1.iloc[_]['count']/len(ts1))[0:6]+',\\n')\n",
    "#     f.close()\n",
    "\n",
    "#     f=open('safe_1000_words_tf.csv','w')\n",
    "#     f.write('word,total_importance,importance_proportion,\\n')\n",
    "#     for _ in range(1000):\n",
    "#         f.write(ts0.iloc[_]['index']+','+str(ts0.iloc[_]['count'])+','+str(ts0.iloc[_]['count']/len(ts0))[0:6]+',\\n')\n",
    "\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47799582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to count total importance of words\n",
    "# count_importance=False\n",
    "# if count_importance:\n",
    "#     tweet_series_tf=pd.Series(name=\"count\", data=0.0, index=tf.get_feature_names())\n",
    "#     for tnum in range(0,len(xtfa)):\n",
    "#         s=pd.Series(name=\"count\", data=xtfa[tnum], index=tf.get_feature_names())\n",
    "#         s=s[s>0]\n",
    "#         s=pd.DataFrame(s)\n",
    "#         s.reset_index(inplace=True)\n",
    "#         for il in range(len(s)):\n",
    "#             tweet_series_tf.at[s['index'].iloc[il]]+=s.iloc[il]['count']\n",
    "\n",
    "\n",
    "#     ts=tweet_series_tf.copy()\n",
    "#     ts.sort_values(ascending=False, inplace=True)\n",
    "#     ts=pd.DataFrame(ts)\n",
    "#     ts.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "#     f=open('500_words_tf.csv','w')\n",
    "#     f.write('word,total_importance,importance_proportion,\\n')\n",
    "#     for _ in range(500):\n",
    "#         f.write(ts.iloc[_]['index']+','+str(ts.iloc[_]['count'])+','+str(ts.iloc[_]['count']/len(ts))[0:6]+',\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I was going to take a random sample before modeling, but I decided to just wait the 20 minutes :/\n",
    "# import random\n",
    "\n",
    "# xtfa_rows=[]\n",
    "\n",
    "# for i in range(len(xtfa)):\n",
    "#     random.seed(i*10)\n",
    "#     if random.random()<0.2:\n",
    "#         xtfa_rows.append(True)\n",
    "#     else:\n",
    "#         xtfa_rows.append(False)\n",
    "        \n",
    "# xtfa_s=xtfa[xtfa_rows]\n",
    "# xtfa_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics=36\n",
    "\n",
    "# text_list=train['tokens'].tolist()\n",
    "# for i in range(len(text_list)):\n",
    "#     text_list[i]=listify(text_list[i])\n",
    "\n",
    "# dictionary=corpora.Dictionary(text_list)\n",
    "# doc_term_matrix = []\n",
    "\n",
    "# for doc in train['tokens']:\n",
    "#     doc_term_matrix.append(dictionary.doc2bow(listify(doc,True)))\n",
    "\n",
    "# doc_word = tf.transform(tokens_all).transpose()\n",
    "# pd.DataFrame(doc_word.toarray(), tf.get_feature_names()).head()\n",
    "\n",
    "# corpus = matutils.Sparse2Corpus(doc_word)\n",
    "# dictionary = corpora.Dictionary()\n",
    "# id2word = dict((v, k) for k, v in tf.vocabulary_.items())\n",
    "\n",
    "\n",
    "# lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # mds = MDS(random_state=0)\n",
    "# # xtfa_md = mds.fit_transform(xtfa)\n",
    "# # print(xtfa_md)\n",
    "# # stress = mds.stress_\n",
    "# # print(stress)\n",
    "\n",
    "# # #Holy crap MDS does NOT scale well at ALL. ~7000 datapoints = 20 minutes.\n",
    "# # #Okay I'm just gonna pickle these because I'm NOT doing that again.\n",
    "\n",
    "# # f = open('xtfa_md', 'wb')\n",
    "# # pickle.dump(xtfa_md, f)\n",
    "# # f.close()\n",
    "\n",
    "# # f = open('mds_s5', 'wb') \n",
    "# # pickle.dump(mds, f)\n",
    "# # f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Repeat for Disaster Tweets only\n",
    "\n",
    "\n",
    "# mds1 = MDS(random_state=0)\n",
    "# xtfa1_md = mds1.fit_transform(xtfa1)\n",
    "# print(xtfa1_md)\n",
    "# stress = mds1.stress_\n",
    "# print(stress)\n",
    "\n",
    "\n",
    "# f = open('xtfa1_md', 'wb')\n",
    "# pickle.dump(xtfa1_md, f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('mds1_s5', 'wb') \n",
    "# pickle.dump(mds1, f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "\n",
    "# # Repeat for Safe Tweets only\n",
    "\n",
    "\n",
    "# mds0 = MDS(random_state=0)\n",
    "# xtfa0_md = mds0.fit_transform(xtfa0)\n",
    "# print(xtfa0_md)\n",
    "# stress = mds0.stress_\n",
    "# print(stress)\n",
    "\n",
    "\n",
    "# f = open('xtfa0_md', 'wb')\n",
    "# pickle.dump(xtfa0_md, f)\n",
    "# f.close()\n",
    "\n",
    "# f = open('mds0_s5', 'wb') \n",
    "# pickle.dump(mds0, f)\n",
    "# f.close()\n",
    "\n",
    "# def display_mds(xtfa_any,xtfa_mds, cnums, clusternum='all'):\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=[10,10])\n",
    "#     if clusternum=='all':\n",
    "#         plt.scatter(xtfa_md[:,0],xtfa_md[:,1],c=cluster_nums)\n",
    "#     else:\n",
    "#         plt.scatter(xtfa_md[:,0],xtfa_md[:,1],alpha=cluster_nums==clusternum,c=cluster_nums)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
